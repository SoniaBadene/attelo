{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers\n",
    "\n",
    "An attelo parser converts “documents” (here: EDUs with some metadata) into graphs (with EDUs as nodes and relation labels between them). In API terms, a parser is something that enriches datapacks, progressively adding or stripping away information until we get a full graph.\n",
    "\n",
    "Parsers follow the scikit-learn estimator and transformer conventions, ie. with a `fit` function to learn some model from training data and a `transform` function to convert (in our case) datapacks to enriched datapacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "To begin our exploration of attelo parsers, let's load up a tiny multipack of sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading edus and pairings... done [1 ms]\n",
      "Reading features... done [1 ms]\n",
      "Build data packs... done [0 ms]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from os import path as fp\n",
    "from attelo.io import (load_multipack)\n",
    "\n",
    "CORPUS_DIR = 'example-corpus'\n",
    "PREFIX = fp.join(CORPUS_DIR, 'tiny')\n",
    "\n",
    "# load the data into a multipack\n",
    "mpack = load_multipack(PREFIX + '.edus',\n",
    "                       PREFIX + '.pairings',\n",
    "                       PREFIX + '.features.sparse',\n",
    "                       PREFIX + '.features.sparse.vocab',\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set aside one of the datapacks to test with, leaving the other two for training.  We do this by hand for this simple example, but you may prefer to use the helper functions in [attelo.fold](../api-doc/attelo#module-attelo.fold) when working with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multipack entries: 3\n",
      "train entries: 2\n"
     ]
    }
   ],
   "source": [
    "test_dpack = mpack.values()[0]\n",
    "train_mpack = {k: mpack[k] for k in mpack.keys()[1:]}\n",
    "\n",
    "print('multipack entries:', len(mpack))\n",
    "print('train entries:', len(train_mpack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a parser out\n",
    "\n",
    "Now that we have our training and test data, we can try feeding them to a simple parser.  Before doing this, we'll take a quick detour to define a helper function to visualise our parse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(dpack):\n",
    "    'summarise parser results'\n",
    "    for i, (edu1, edu2) in enumerate(dpack.pairings):\n",
    "        wanted = dpack.get_label(dpack.target[i])\n",
    "        got = dpack.get_label(dpack.graph.prediction[i])\n",
    "        print(i, edu1.id, edu2.id, '\\t|', got, '\\twanted:', wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for parsing, we'll start with the attachment pipeline. It combines a [learner](../api-doc/attelo.learning) with a [decoder](../api-doc/attelo.decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ROOT d2_e2 \t| __UNK__ \twanted: elaboration\n",
      "1 d2_e3 d2_e2 \t| UNRELATED \twanted: narration\n",
      "2 d2_e4 d2_e2 \t| UNRELATED \twanted: UNRELATED\n",
      "3 ROOT d2_e3 \t| UNRELATED \twanted: continuation\n",
      "4 d2_e2 d2_e3 \t| __UNK__ \twanted: narration\n",
      "5 d2_e4 d2_e3 \t| UNRELATED \twanted: narration\n",
      "6 ROOT d2_e4 \t| UNRELATED \twanted: UNRELATED\n",
      "7 d2_e3 d2_e4 \t| __UNK__ \twanted: elaboration\n",
      "8 d2_e2 d2_e4 \t| UNRELATED \twanted: UNRELATED\n"
     ]
    }
   ],
   "source": [
    "from attelo.decoding.baseline import (LastBaseline)\n",
    "from attelo.learning import (SklearnAttachClassifier)\n",
    "from attelo.parser.attach import (AttachPipeline)\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "\n",
    "learner = SklearnAttachClassifier(LogisticRegression())\n",
    "decoder = LastBaseline()\n",
    "parser1 = AttachPipeline(learner=learner, \n",
    "                         decoder=decoder)\n",
    "\n",
    "# train the parser\n",
    "train_dpacks = train_mpack.values()\n",
    "train_targets = [x.target for x in train_dpacks]\n",
    "parser1.fit(train_dpacks, train_targets)\n",
    "        \n",
    "# now run on a test pack\n",
    "dpack = parser1.transform(test_dpack)\n",
    "print_results(dpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Trying another parser\n",
    "\n",
    "In the output above, our predictions for every edge are either `__UNK__` or `UNRELATED`.  The attachment pipeline only  predicts if edges will be attached or not.  What we need is to be able to predict their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ROOT d2_e2 \t| elaboration \twanted: elaboration\n",
      "1 d2_e3 d2_e2 \t| elaboration \twanted: narration\n",
      "2 d2_e4 d2_e2 \t| narration \twanted: UNRELATED\n",
      "3 ROOT d2_e3 \t| elaboration \twanted: continuation\n",
      "4 d2_e2 d2_e3 \t| elaboration \twanted: narration\n",
      "5 d2_e4 d2_e3 \t| narration \twanted: narration\n",
      "6 ROOT d2_e4 \t| elaboration \twanted: UNRELATED\n",
      "7 d2_e3 d2_e4 \t| elaboration \twanted: elaboration\n",
      "8 d2_e2 d2_e4 \t| narration \twanted: UNRELATED\n"
     ]
    }
   ],
   "source": [
    "from attelo.learning import (SklearnLabelClassifier)\n",
    "from attelo.parser.label import (SimpleLabeller)\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "\n",
    "learner = SklearnLabelClassifier(LogisticRegression())\n",
    "parser2 = SimpleLabeller(learner=learner)\n",
    "\n",
    "# train the parser\n",
    "parser2.fit(train_dpacks, train_targets)\n",
    "        \n",
    "# now run on a test pack\n",
    "dpack = parser2.transform(test_dpack)\n",
    "print_results(dpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't quite look right.  Now we have labels, but none of our edges are `UNRELATED`.  But this is because the simple labeller will apply labels on all unknown edges.  What we need is to be able to combine the attach and label parsers in a parsing pipeline\n",
    "\n",
    "## Parsing pipeline\n",
    "\n",
    "A parsing pipeline is a parser that combines other parsers in sequence.  For purposes of learning/fitting, the individual steps can be thought of as being run in parallel (in practice, they are fitted in sequnce).  For transforming though, they are run in order.  A pipeline thus refines a datapack over the course of multiple parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ROOT d2_e2 \t| elaboration \twanted: elaboration\n",
      "1 d2_e3 d2_e2 \t| UNRELATED \twanted: narration\n",
      "2 d2_e4 d2_e2 \t| UNRELATED \twanted: UNRELATED\n",
      "3 ROOT d2_e3 \t| UNRELATED \twanted: continuation\n",
      "4 d2_e2 d2_e3 \t| elaboration \twanted: narration\n",
      "5 d2_e4 d2_e3 \t| UNRELATED \twanted: narration\n",
      "6 ROOT d2_e4 \t| UNRELATED \twanted: UNRELATED\n",
      "7 d2_e3 d2_e4 \t| elaboration \twanted: elaboration\n",
      "8 d2_e2 d2_e4 \t| UNRELATED \twanted: UNRELATED\n"
     ]
    }
   ],
   "source": [
    "from attelo.parser.pipeline import (Pipeline)\n",
    "\n",
    "# this is actually attelo.parser.full.PostlabelPipeline\n",
    "parser3 = Pipeline(steps=[('attach', parser1),\n",
    "                          ('label', parser2)])\n",
    "\n",
    "parser3.fit(train_dpacks, train_targets)\n",
    "dpack = parser3.transform(test_dpack)\n",
    "print_results(dpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (for now)\n",
    "\n",
    "We have now seen some basic attelo parsers, how they use the scikit-learn fit/transform idiom, and we can combine them with pipelines.  In future tutorials we'll break some of the parsers down into their constituent parts (notice the attach pipeline is itself a pipeline) and explore the process of writing parsers of our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
